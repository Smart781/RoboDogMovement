using UnityEngine;
using Unity.MLAgents;
using Unity.MLAgents.Sensors;
using Unity.MLAgents.Actuators;
using System.Collections;
using System;
using Random = UnityEngine.Random;
using UnityEngine.InputSystem;

public class DoggyAgent : Agent
{
    [Header("Сервоприводы")]
    public ArticulationBody[] legs;

    [Header("Скорость работы сервоприводов")]
    public float servoSpeed;

    [Header("Тело")]
    public ArticulationBody body;
    private Vector3 defPos;
    private Quaternion defRot;
    public float strenghtMove;

    [Header("Куб (цель)")]
    public GameObject cube;

    [Header("Сенсоры")]
    public Unity.MLAgentsExamples.GroundContact[] groundContacts;

    private float distToTarget = 0f;

    //private Oscillator m_Oscillator;

    public override void Initialize()
    {
        distToTarget = Vector3.Distance(body.transform.position, cube.transform.position);
        defRot = body.transform.rotation;
        defPos = body.transform.position;

        //m_Oscillator = GetComponent<Oscillator>(); ***
        //m_Oscillator.ManagedReset(); ***
    }

    public void ResetDog()
    {
        Quaternion newRot = Quaternion.Euler(-90, 0, Random.Range(0f, 360f));


        body.TeleportRoot(defPos, newRot);
        //body.TeleportRoot(defPos, defRot); ***
        body.velocity = Vector3.zero;
        body.angularVelocity = Vector3.zero;

        for (int i = 0; i < 12; i++)
        {
            //MoveLeg(legs[i], Random.Range(legs[i].xDrive.lowerLimit, legs[i].xDrive.upperLimit));
            MoveLeg(legs[i], 0);
        }
    }

    public override void OnEpisodeBegin()
    {
        ResetDog();
        //m_Oscillator.ManagedReset(); ***

        //cube.transform.position = new Vector3(5, 0.21f, Random.Range(-2f, 2f));
        cube.transform.position = new Vector3(Random.Range(-7.5f, 7.5f), 0.21f, Random.Range(-7.5f, 7.5f));
        //cube.transform.position = new Vector3(5f, 0.21f, 0); ***

        //cube.transform.position = new Vector3(8f, 0.26f, 0f);
    }

    public override void CollectObservations(VectorSensor sensor)
    {
        sensor.AddObservation(body.transform.position);
        sensor.AddObservation(body.velocity);
        sensor.AddObservation(body.angularVelocity);
        sensor.AddObservation(body.transform.right);

        // Позиция куба
        sensor.AddObservation(cube.transform.position);

        // Относительное положение куба
        Vector3 relativePosition = cube.transform.position - body.transform.position;
        sensor.AddObservation(relativePosition);

        // Угловая позиция куба
        Vector3 toCube = (cube.transform.position - body.transform.position).normalized;
        float angleToCube = Vector3.SignedAngle(body.transform.right, toCube, Vector3.up);
        sensor.AddObservation(angleToCube);

        // Расстояние до куба
        float distanceToCube = Vector3.Distance(body.transform.position, cube.transform.position);
        sensor.AddObservation(distanceToCube);
        foreach (var leg in legs)
        {
            sensor.AddObservation(leg.xDrive.target);
            sensor.AddObservation(leg.velocity);
            sensor.AddObservation(leg.angularVelocity);
        }

        foreach(var groundContact in groundContacts)
        {
            sensor.AddObservation(groundContact.touchingGround);
        }
    }

    public override void OnActionReceived(ActionBuffers vectorAction)
    {
        var actions = vectorAction.ContinuousActions;
        for (int i = 0; i < 12; i++)
        {
            float angle = Mathf.Lerp(legs[i].xDrive.lowerLimit, legs[i].xDrive.upperLimit, (actions[i] + 1) * 0.5f);
            MoveLeg(legs[i], angle);
        }

        //m_Oscillator.ManagedUpdate(); ***

        float currentDistanceToTarget = Vector3.Distance(body.transform.position, cube.transform.position);
        float distanceReward = distToTarget - currentDistanceToTarget;
        //AddReward(distanceReward);
        distToTarget = currentDistanceToTarget;

        if (currentDistanceToTarget < 1f)
        {
            AddReward(50.0f);
            EndEpisode();
        }

        if (distanceReward < 0)
        {
            AddReward(-0.01f);
        }

        if (body.velocity.magnitude < 0.1f)
        {
            AddReward(-0.01f);
        }
    }

    public override void Heuristic(in ActionBuffers actionsOut)
    {
        ActionSegment<float> continuousActions = actionsOut.ContinuousActions;
        //Debug.Log("Upper Limit of legs[0]: " + legs[6].xDrive.upperLimit);
        //MoveLeg(legs[6], legs[6].xDrive.upperLimit);
        continuousActions[0] = Input.GetAxisRaw("Horizontal");
        continuousActions[1] = Input.GetAxisRaw("Vertical");
    }
    
    public void FixedUpdate()
    {
        // body.AddForce((cube.transform.position - body.transform.position).normalized * strenghtMove);
        // for (int i = 0; i < 12; i++)
        // {
        //    legs[i].AddForce((cube.transform.position - body.transform.position).normalized * strenghtMove / 20f);
        // }

        RaycastHit hit;
        if (Physics.Raycast(body.transform.position, body.transform.right, out hit))
        {
            if (hit.collider.gameObject == cube)
            {
                AddReward(1f);
                // body.AddForce(2f * strenghtMove * (cube.transform.position - body.transform.position).normalized);
                // for (int i = 0; i < 12; i++)
                // {
                //     legs[i].AddForce((cube.transform.position - body.transform.position).normalized * strenghtMove / 10f);
                // }
            }
            else
            {
                AddReward(-0.001f);
            }
        }
        Debug.DrawRay(body.transform.position, body.transform.right, Color.white);
    }

    void MoveLeg(ArticulationBody leg, float targetAngle)
    {
        leg.GetComponent<Leg>().MoveLeg(targetAngle, servoSpeed);
    }
}





(venv) C:\ML-Agents\ml-agents-develop\ml-agents-develop\Project>mlagents-learn config/AmyImitation.yaml --run-id=Doggy5
C:\Users\123\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\tensor\python_tensor.cpp:434.)
  _C._set_default_tensor_type(t)

            ┐  ╖
        ╓╖╬│╡  ││╬╖╖
    ╓╖╬│││││┘  ╬│││││╬╖
 ╖╬│││││╬╜        ╙╬│││││╖╖                               ╗╗╗
 ╬╬╬╬╖││╦╖        ╖╬││╗╣╣╣╬      ╟╣╣╬    ╟╣╣╣             ╜╜╜  ╟╣╣
 ╬╬╬╬╬╬╬╬╖│╬╖╖╓╬╪│╓╣╣╣╣╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╒╣╣╖╗╣╣╣╗   ╣╣╣ ╣╣╣╣╣╣ ╟╣╣╖   ╣╣╣
 ╬╬╬╬┐  ╙╬╬╬╬│╓╣╣╣╝╜  ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╣╙ ╙╣╣╣  ╣╣╣ ╙╟╣╣╜╙  ╫╣╣  ╟╣╣
 ╬╬╬╬┐     ╙╬╬╣╣      ╫╣╣╣╬      ╟╣╣╬    ╟╣╣╣ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣     ╣╣╣┌╣╣╜
 ╬╬╬╜       ╬╬╣╣      ╙╝╣╣╬      ╙╣╣╣╗╖╓╗╣╣╣╜ ╟╣╣╬   ╣╣╣  ╣╣╣  ╟╣╣╦╓    ╣╣╣╣╣
 ╙   ╓╦╖    ╬╬╣╣   ╓╗╗╖            ╙╝╣╣╣╣╝╜   ╘╝╝╜   ╝╝╝  ╝╝╝   ╙╣╣╣    ╟╣╣╣
   ╩╬╬╬╬╬╬╦╦╬╬╣╣╗╣╣╣╣╣╣╣╝                                             ╫╣╣╣╣
      ╙╬╬╬╬╬╬╬╣╣╣╣╣╣╝╜
          ╙╬╬╬╣╣╣╜
             ╙

 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 2.5.1+cpu
C:\Users\123\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\tensor\python_tensor.cpp:434.)
  _C._set_default_tensor_type(t)
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[INFO] Connected to Unity environment with package version 3.0.0-exp.1 and communication version 1.5.0
[INFO] Connected new brain: AmyImitation?team=0
[INFO] Hyperparameters for behavior name AmyImitation:
        trainer_type:   ppo
        hyperparameters:
          batch_size:   128
          buffer_size:  2048
          learning_rate:        0.0003
          beta: 0.0005
          epsilon:      0.2
          lambd:        0.95
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       constant
          beta_schedule:        constant
          epsilon_schedule:     constant
        network_settings:
          normalize:    True
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals:
          extrinsic:
            gamma:      0.99
            strength:   1.0
            network_settings:
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       100
        checkpoint_interval:    50000
        max_steps:      1000000
        time_horizon:   64
        summary_freq:   15000
        threaded:       True
        self_play:      None
        behavioral_cloning:
          demo_path:    Assets/Demonstrations/AmyIm2.demo
          steps:        1000000
          strength:     0.5
          samples_per_update:   0
          num_epoch:    None
          batch_size:   None
C:\Users\123\AppData\Local\Programs\Python\Python39\lib\site-packages\mlagents\trainers\torch_entities\utils.py:289: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\TensorShape.cpp:3687.)
  torch.nn.functional.one_hot(_act.T, action_size[i]).float()
[INFO] AmyImitation. Step: 15000. Time Elapsed: 206.091 s. Mean Reward: 0.301. Std of Reward: 7.349. Training.
[INFO] AmyImitation. Step: 30000. Time Elapsed: 401.755 s. Mean Reward: 0.944. Std of Reward: 9.891. Training.
[INFO] AmyImitation. Step: 45000. Time Elapsed: 623.232 s. Mean Reward: 0.594. Std of Reward: 9.139. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-49990.onnx
[INFO] AmyImitation. Step: 60000. Time Elapsed: 849.373 s. Mean Reward: 1.286. Std of Reward: 10.427. Training.
[INFO] AmyImitation. Step: 75000. Time Elapsed: 1063.413 s. Mean Reward: 1.401. Std of Reward: 12.211. Training.
[INFO] AmyImitation. Step: 90000. Time Elapsed: 1277.486 s. Mean Reward: 1.550. Std of Reward: 17.225. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-99953.onnx
[INFO] AmyImitation. Step: 105000. Time Elapsed: 3219.057 s. Mean Reward: 1.004. Std of Reward: 14.492. Training.
[INFO] AmyImitation. Step: 120000. Time Elapsed: 3383.294 s. Mean Reward: 2.070. Std of Reward: 13.300. Training.
[INFO] AmyImitation. Step: 135000. Time Elapsed: 3562.844 s. Mean Reward: 1.323. Std of Reward: 10.366. Training.
[INFO] AmyImitation. Step: 150000. Time Elapsed: 3742.616 s. Mean Reward: 4.184. Std of Reward: 16.952. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-149937.onnx
[INFO] AmyImitation. Step: 165000. Time Elapsed: 3926.686 s. Mean Reward: 12.146. Std of Reward: 55.471. Training.
[INFO] AmyImitation. Step: 180000. Time Elapsed: 4160.277 s. Mean Reward: 13.867. Std of Reward: 40.214. Training.
[INFO] AmyImitation. Step: 195000. Time Elapsed: 4407.057 s. Mean Reward: 18.853. Std of Reward: 46.196. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-199962.onnx
[INFO] AmyImitation. Step: 210000. Time Elapsed: 4654.356 s. Mean Reward: 29.540. Std of Reward: 77.424. Training.
[INFO] AmyImitation. Step: 225000. Time Elapsed: 4898.572 s. Mean Reward: 23.289. Std of Reward: 50.807. Training.
[INFO] AmyImitation. Step: 240000. Time Elapsed: 5128.453 s. Mean Reward: 117.451. Std of Reward: 208.295. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-249987.onnx
[INFO] AmyImitation. Step: 255000. Time Elapsed: 5354.361 s. Mean Reward: 89.556. Std of Reward: 160.653. Training.
[INFO] AmyImitation. Step: 270000. Time Elapsed: 5579.120 s. Mean Reward: 54.664. Std of Reward: 154.160. Training.
[INFO] AmyImitation. Step: 285000. Time Elapsed: 5786.566 s. Mean Reward: 152.163. Std of Reward: 176.409. Training.
[INFO] AmyImitation. Step: 300000. Time Elapsed: 5980.922 s. Mean Reward: 106.703. Std of Reward: 263.255. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-299950.onnx
[INFO] AmyImitation. Step: 315000. Time Elapsed: 6177.484 s. Mean Reward: 56.435. Std of Reward: 109.458. Training.
[INFO] AmyImitation. Step: 330000. Time Elapsed: 6372.786 s. Mean Reward: -0.791. Std of Reward: 17.313. Training.
[INFO] AmyImitation. Step: 345000. Time Elapsed: 6567.483 s. Mean Reward: 24.166. Std of Reward: 81.722. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-349971.onnx
[INFO] AmyImitation. Step: 360000. Time Elapsed: 6762.433 s. Mean Reward: 460.118. Std of Reward: 580.895. Training.
[INFO] AmyImitation. Step: 375000. Time Elapsed: 6959.118 s. Mean Reward: 330.955. Std of Reward: 397.230. Training.
[INFO] AmyImitation. Step: 390000. Time Elapsed: 7153.001 s. Mean Reward: 16.904. Std of Reward: 62.236. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-399954.onnx
[INFO] AmyImitation. Step: 405000. Time Elapsed: 7347.800 s. Mean Reward: 33.792. Std of Reward: 43.434. Training.
[INFO] AmyImitation. Step: 420000. Time Elapsed: 7541.757 s. Mean Reward: 526.134. Std of Reward: 488.253. Training.
[INFO] AmyImitation. Step: 435000. Time Elapsed: 7737.977 s. Mean Reward: 165.603. Std of Reward: 260.439. Training.
[INFO] AmyImitation. Step: 450000. Time Elapsed: 7932.099 s. Mean Reward: 260.744. Std of Reward: 364.953. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-449964.onnx
[INFO] AmyImitation. Step: 465000. Time Elapsed: 8127.415 s. Mean Reward: 21.022. Std of Reward: 42.430. Training.
[INFO] AmyImitation. Step: 480000. Time Elapsed: 8324.587 s. Mean Reward: 183.939. Std of Reward: 245.699. Training.
[INFO] AmyImitation. Step: 495000. Time Elapsed: 8520.551 s. Mean Reward: 329.578. Std of Reward: 614.911. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-499945.onnx
[INFO] AmyImitation. Step: 510000. Time Elapsed: 8730.068 s. Mean Reward: 487.285. Std of Reward: 760.023. Training.
[INFO] AmyImitation. Step: 525000. Time Elapsed: 8945.713 s. Mean Reward: 227.654. Std of Reward: 159.850. Training.
[INFO] AmyImitation. Step: 540000. Time Elapsed: 9151.443 s. Mean Reward: 642.321. Std of Reward: 704.283. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-549962.onnx
[INFO] AmyImitation. Step: 555000. Time Elapsed: 9387.296 s. Mean Reward: 396.035. Std of Reward: 271.648. Training.
[INFO] AmyImitation. Step: 570000. Time Elapsed: 9620.930 s. Mean Reward: 356.100. Std of Reward: 378.632. Training.
[INFO] AmyImitation. Step: 585000. Time Elapsed: 9854.226 s. Mean Reward: 1321.954. Std of Reward: 794.162. Training.
[INFO] AmyImitation. Step: 600000. Time Elapsed: 10085.801 s. Mean Reward: 466.423. Std of Reward: 540.177. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-599993.onnx
[INFO] AmyImitation. Step: 615000. Time Elapsed: 10316.452 s. Mean Reward: 640.285. Std of Reward: 434.714. Training.
[INFO] AmyImitation. Step: 630000. Time Elapsed: 10548.570 s. Mean Reward: 1138.095. Std of Reward: 918.085. Training.
[INFO] AmyImitation. Step: 645000. Time Elapsed: 10779.406 s. Mean Reward: 62.046. Std of Reward: 138.503. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-649995.onnx
[INFO] AmyImitation. Step: 660000. Time Elapsed: 11012.067 s. Mean Reward: 107.159. Std of Reward: 304.486. Training.
[INFO] AmyImitation. Step: 675000. Time Elapsed: 11224.701 s. Mean Reward: 379.042. Std of Reward: 641.268. Training.
[INFO] AmyImitation. Step: 690000. Time Elapsed: 11443.044 s. Mean Reward: 86.161. Std of Reward: 224.187. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-699953.onnx
[INFO] AmyImitation. Step: 705000. Time Elapsed: 11655.002 s. Mean Reward: 237.251. Std of Reward: 444.697. Training.
[INFO] AmyImitation. Step: 720000. Time Elapsed: 11866.951 s. Mean Reward: 185.685. Std of Reward: 378.499. Training.
[INFO] AmyImitation. Step: 735000. Time Elapsed: 12081.492 s. Mean Reward: 709.232. Std of Reward: 819.711. Training.
[INFO] AmyImitation. Step: 750000. Time Elapsed: 12295.351 s. Mean Reward: 76.613. Std of Reward: 244.140. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-749945.onnx
[INFO] AmyImitation. Step: 765000. Time Elapsed: 12513.652 s. Mean Reward: 310.454. Std of Reward: 569.003. Training.
[INFO] AmyImitation. Step: 780000. Time Elapsed: 12727.506 s. Mean Reward: 281.705. Std of Reward: 783.947. Training.
[INFO] AmyImitation. Step: 795000. Time Elapsed: 12941.525 s. Mean Reward: 30.364. Std of Reward: 143.991. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-799960.onnx
[INFO] AmyImitation. Step: 810000. Time Elapsed: 13155.908 s. Mean Reward: 56.931. Std of Reward: 201.212. Training.
[INFO] AmyImitation. Step: 825000. Time Elapsed: 13370.474 s. Mean Reward: 89.661. Std of Reward: 327.875. Training.
[INFO] AmyImitation. Step: 840000. Time Elapsed: 13584.836 s. Mean Reward: 310.913. Std of Reward: 1004.223. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-849973.onnx
[INFO] AmyImitation. Step: 855000. Time Elapsed: 13802.105 s. Mean Reward: 116.942. Std of Reward: 376.427. Training.
[INFO] AmyImitation. Step: 870000. Time Elapsed: 14015.774 s. Mean Reward: 167.728. Std of Reward: 559.563. Training.
[INFO] AmyImitation. Step: 885000. Time Elapsed: 14229.175 s. Mean Reward: 375.197. Std of Reward: 959.201. Training.
[INFO] AmyImitation. Step: 900000. Time Elapsed: 14444.531 s. Mean Reward: 61.729. Std of Reward: 273.543. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-899976.onnx
[INFO] AmyImitation. Step: 915000. Time Elapsed: 14659.445 s. Mean Reward: 5.283. Std of Reward: 48.390. Training.
[INFO] AmyImitation. Step: 930000. Time Elapsed: 14874.237 s. Mean Reward: 35.865. Std of Reward: 114.865. Training.
[INFO] AmyImitation. Step: 945000. Time Elapsed: 15092.324 s. Mean Reward: 285.945. Std of Reward: 722.251. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-949956.onnx
[INFO] AmyImitation. Step: 960000. Time Elapsed: 15307.001 s. Mean Reward: 67.885. Std of Reward: 306.603. Training.
[INFO] AmyImitation. Step: 975000. Time Elapsed: 15549.279 s. Mean Reward: 69.692. Std of Reward: 237.786. Training.
[INFO] AmyImitation. Step: 990000. Time Elapsed: 15783.729 s. Mean Reward: 23.064. Std of Reward: 128.127. Training.
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-999949.onnx
[INFO] Exported results\Doggy5\AmyImitation\AmyImitation-1000013.onnx
[INFO] Copied results\Doggy5\AmyImitation\AmyImitation-1000013.onnx to results\Doggy5\AmyImitation.onnx.

